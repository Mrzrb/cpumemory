# 现代硬件

> 我们现在大部分服务都是依托软件的，而软件是依托硬件的，所以硬件是非常重要的。

  现今我们谈论‘扩容’一般都是指横向扩容，通过增加集群中互联的机器来增加集群的性能，而不是增强单台服务器的性能，现在网络成本很低并且速度上也有一定的保障，这种方案成为人们最具性价比的首选。当然也不是说超级大型机没有用武之地了，对于超大型计算来说，超大型机器仍具有不可取代的地位。对于商用方案，小型商用硬件占据了绝大部分市场。2007年，Red Hat
  描述了一种数据中心的标准工作单元--一种四通道四核CPU计算器,并且具有超线程的能力，这意味着这种计算机最大拥有64个虚拟的处理器。这种能力超大型计算器也可以提供，但是我们这种方案具有更高的性价比和更大的优化空间。

![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190905203446.png)

  商用计算机之间存在这很大的不同，我们只考虑其中最重要的不同之处，这些技术更新换代非常快，读者在阅读的时候要带入实际考虑。

这些年个人电脑芯片的规范是拥有北桥和南桥两部分，如下图所示：

![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190905204041.png)

CPU之前通过北桥上的FSB连接(现在时下的北桥已经没有了，多核CPU被焊接在一个芯片上)，北桥上还有内存控制器(memory controller) 和RAM芯片。不同的RAM，比如DRAM、 Rambus、 还有SDRAM 需要不同的内存控制器来支持。

为了能让CPU控制其他设备，北桥必须和南桥相连。 **南桥**(Southbridge) 又叫做IO桥，管理着大量不同的总线设备，比如PCI, PCI-E(PCI Express), SATA, USB等。老一点的系统上加速图形接口(AGP slot --- Accelerated Graphics Port)是直接插在北桥上的，现在都通过PCI-E插在南桥上。

这种构架有如下一些值得一提的点:
 - CPU之间的数据流必须通过北桥
 - CPU和RAM指教的数据流必须通过北桥
 - RAM只有一个数据接口
 - CPU控制南桥设备必须经过北桥

这种架构有一些设计上的缺陷(瓶颈):

 - 比如南桥上的设备想获取RAM里面的数据，数据流必须先经过CPU，明显会使得系统性能整体下降，(想想也是，CPU正事不干，净送快递了#(滑稽))，这么干明显不行啊，于是出现了一种叫DMA(Direct memory access)的技术,DMA允许设备通过北桥直接存取数据，而不需要经过CPU的参与，(明显解放了CPU的生产力,加鸡腿！)
如今任何总线的高性能设备都可以使用DMA，这大大减少了CPU的负载，但是来自DMA的数据流会和来自CPU的数据流争夺北桥的带宽，这个因素是不容忽略的。

 - 第二个瓶颈是在北桥和RAM之间的通路，老的设备上他俩之间只有一条总线，所以并行是不可能的。近几年的RAM和北桥之间有两条总线(或者channel， 称之为DDR2)，通过这种方法提高了1倍带宽，现在人们说的DDR4其实就是在之前的基础上加了更多的bus。下图为DDR4 SDRAM。

在有限的带宽下，低延迟的内存调度显得尤为重要。CPU比内存快很多倍，因此CPU必须等待内存来获取数据，尽管使用了CPU cache。多核、多线程在同时获取内存的时候等待时间可能会更长，DMA也是一样的。
![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190905220810.png)

在一些系统里面北桥不包含内存控制器，北桥连接了一些外置的内存控制器 如下图所示
![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190905223758.png)

这种构架的好处是有多个内存总线因此内存宽带提升了，同时多处理器在取并行取内存的能力增强了。

除此之外，还可以通过把内存控制器嵌入到CPU中来提高内存带宽。AMD的SMP系统就是使用了这种构架。下图展示了这种构架的示意图：

![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190906204743.png)

这种构架下，一个4核CPU即可获得比传统北桥构架高数倍的内存带宽。然而，这种构架也有缺点，系统必须保证所有的内存都可以被每一个CPU存取， 当某一个CPU要在和别的CPU相连的内存时，必须通过中间CPU的协作才可以，当同时需要和多个CPU上的内存交互的时候，之前讨论的无DMA构架的劣势仿佛又出现了。

上面出现的情况中，CPU转发内存上的数据的时候，会造成性能上的浪费。我们把这种消耗叫做"NUMA factor"。
![](http://zhangrb-image.oss-cn-beijing.aliyuncs.com/image/20190912231102.png)
上图所示的构架中的CPU获取连接在别的CPU上的内存的数据时候， 有两种： 一种是相邻CPU的内存， 另外一种是对角CPU的(中间需要一个CPU来转发)。这种构架下，构架设计越复杂， 这种复杂度越高。IBM有一种构架设计，将CPU们设计成CPU簇，一个簇内的CPU互相访问内存NUMA影响很小， 而跨簇消耗则是巨大的。

现代商业机大部分采用NUMA构架，未来NUMA构架还将发挥更大的作用。理解NUMA构架是非常重要的，第五部分我们将讨论更多NUMA相关的构架，以及Linux内核提供的和NUMA相关的技术。

本小节的剩余部分，我们将讨论影响RAM性能的一些具体的细节。除此之外，还有一些非软件层面的因素，本章将不会讨论。感兴趣的读者可以去2.1节，这一节会让你对RAM有一个更完整地把握。

接下来的两个小节，我们即将讨论RAM的硬件细节和memory controller 和 DRAM 芯片组之间的访问接口。读者将会明白RAM的工作方式，以及RAM为啥要这么设计。
